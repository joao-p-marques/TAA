{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab 3 - Logistic Regression\n",
    "\n",
    "## PART 1: Unregularized Logistic Regression ##\n",
    "\n",
    "**Objectives**: Implement Unregularized Logistic Regression and get to see it works on data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** Build a Logistic Regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and you want to determine each applicant's chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for Logistic Regression. For each training example, you have the applicant's scores on two exams and the admissions decision. \n",
    "\n",
    "Your task is to build a classification model that estimates an applicant's probability of admission based on the scores from those two exams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The file *ex2data1.txt* contains the dataset for this problem. The 1st and the 2nd columns are the scores from the exams (X), the 3rd column (y) indicates if the student was admitted (1) or not admitted (0). \n",
    "\n",
    "Load data into the variable df (e.g. using function pd.read_csv from panda library) and then extract X (the features) and y (the labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ex2data1.txt', header=None)\n",
    "data_n = data.values\n",
    "# print(data)\n",
    "\n",
    "X=data_n[:, :2]\n",
    "#print(X)\n",
    "\n",
    "m, n = X.shape[0], X.shape[1]\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "y=data_n[:, 2]\n",
    "#print(y)\n",
    "\n",
    "y = y.reshape(m, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1  2\n",
       "0  34.623660  78.024693  0\n",
       "1  30.286711  43.894998  0\n",
       "2  35.847409  72.902198  0\n",
       "3  60.182599  86.308552  1\n",
       "4  79.032736  75.344376  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a few examples from the dataset \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data\n",
    "Create a scatter plot of data similar to Fig.1 (using plt.scatter). Students with higher test score for both exam were admitted into the university as expected.\n",
    "\n",
    "<img src=\"images/f1.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : **file ex2data1.txt** </center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8500b44d50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos=(y==1)\n",
    "neg=(y==0)\n",
    "plt.scatter(X[pos[:,0],0],X[pos[:,0],1],c=\"r\",marker=\"+\")\n",
    "plt.scatter(X[neg[:,0],0],X[neg[:,0],1],c=\"b\",marker=\"o\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "Complete *sigmoid* function that computes $ g(z) = \\frac{1}{(1+e^{-z})}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    return the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    gz=1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the sigmoid function for z=0 => ANSWER =0.5 \n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost Function and Gradient\n",
    "\n",
    "Recall that the Logistic Regression model is defined as:    $h_{\\theta}(x^{(i)})=  \\frac{1}{1+e^{-\\theta (x^{(i)})}}$\n",
    "\n",
    "The cost function in Logistic Regression is: $J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [ -y^{(i)}log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)})log(1 - (h_{\\theta}(x^{(i)}))]$\n",
    "\n",
    "The gradient of $J(\\theta)$ is a vector of the same length as $\\theta$  where the jth element (for j = 0, 1,â€¦. n) is defined as:\n",
    "$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "Complete function *costFunction* to return $J(\\theta)$ and the gradient ((partial derivative of $J(\\theta)$ with respect to each $\\theta$) for logistic regression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y):\n",
    "    \"\"\"\n",
    "    Takes in numpy array theta, x and y and return the logistic regression cost function and gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    #number of training examples \n",
    "    m=len(y)\n",
    "    \n",
    "    #vector of the model predictions for all training examples   \n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "         \n",
    "    error = (-y * np.log(h)) - ((1-y)*np.log(1-h))\n",
    "\n",
    "    #cost function\n",
    "    cost = 1/m * sum(error)\n",
    "       \n",
    "    #vector of gradients of all model parameters theta   \n",
    "    grad = 1/m * np.dot(X.transpose(),(h - y))\n",
    "    \n",
    "    return cost[0] , grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Feature normalization\n",
    "Apply the same normalization as in Lab 2 (PART 2 Multivariable Linear Regression). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(X):\n",
    "    \"\"\"\n",
    "    Take in numpy array of X values and return normalize X values,\n",
    "    the mean and standard deviation of each feature\n",
    "    \"\"\"\n",
    "    mean= np.mean(X, axis=0)\n",
    "    std= np.std(X, axis=0)\n",
    "    \n",
    "    X_norm = (X - mean) / std\n",
    "    \n",
    "    return X_norm , mean , std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of initial theta is 0.693\n",
      "Gradient at initial theta (zeros): [[-0.1       ]\n",
      " [-0.28122914]\n",
      " [-0.25098615]]\n"
     ]
    }
   ],
   "source": [
    "#Run featureNormalization to normalize X, store the means and stds.\n",
    "\n",
    "X, X_mean, X_std = featureNormalization(X)\n",
    "\n",
    "#After normalizing the features, add an extra column of 1's corresponding to x0 = 1.\n",
    "X= np.append(np.ones((m, 1)), X, axis=1)\n",
    "\n",
    "\n",
    "# Inicialize vector theta = 0\n",
    "initial_theta = np.zeros((n+1, 1))\n",
    "\n",
    "#Run costFunction\n",
    "cost, grad= costFunction(initial_theta, X, y)\n",
    "\n",
    "print(\"Cost of initial theta is\",round(cost,3) )   # ANSWER: Cost of initial theta is 0.693\n",
    "print(\"Gradient at initial theta (zeros):\",grad)  #ANSWER: Gradient at initial theta (zeros): [[-0.1 ] [-0.28122914] [-0.25098615]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Implement gradient descent in the function *gradientDescent*. \n",
    "The gradient descent algorithm is very similar to linear regression. \n",
    "\n",
    "The only difference is that the hypothesis is now the sigmoid function:  $h_{\\theta}(x)=  \\frac{1}{1+e^{-\\theta^T x}}$\n",
    "\n",
    "The loop structure is written, you need to supply the updates for $\\theta$  within each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    #number of training examples\n",
    "    m=len(y)\n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        cost, grad = ?\n",
    "        theta = ?\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta , J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradientDescent with learning rate 0.5 and 400 iterations. \n",
    "\n",
    "theta , J_history = ?\n",
    "\n",
    "print(\"Theta optimized by gradient descent:\",theta)\n",
    "\n",
    "print(\"The cost for the optimized theta:\",round(J_history[-1],3))  #ANSWER: The cost for the optimized theta: 0.205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Cost Function \n",
    "Choose 400 iterations. Try different values of the learning  rate = [0.01, 0.1, 0.5, 1]\n",
    "and get plots similar to Fig. 2. \n",
    "\n",
    "<img src=\"images/f6.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 2** : **Cost function evolution for varying learning rates ** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=[0.01, 0.1, 0.5, 1]\n",
    "for i in lr:\n",
    "    ?\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the decision boundary\n",
    "   \n",
    "Our model is sigmoid function:  $h_{\\theta}(x)=  \\frac{1}{1+e^{-\\theta^T x}}$\n",
    "\n",
    "If $h_\\theta(x) > 0.5$ => predict class \"1\", that is $\\theta^Tx> 0$ => predict class \"1\"\n",
    "\n",
    "If $h_\\theta(x) < 0.5$ => predict class \"0\", that is $\\theta^Tx< 0$ => predict class \"0\" \n",
    "\n",
    "$\\theta^Tx = 0$  is the decision boundary. \n",
    "\n",
    "In this particular case $\\theta_0 + \\theta_1x_1 + \\theta_2x_2 = 0$ is the decision boundary-   \n",
    "\n",
    "Since, we plot $x_1$ against $x_2$, the boundary line will be the equation $ x_2 = \\frac{-(\\theta_0+\\theta_1x_1)}{\\theta_2}$\n",
    "\n",
    "Plot the data and the decision boundary. You should get a figure similar to Fig.3.\n",
    "\n",
    "<img src=\"images/f2.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 3** : **Training data vs Decision boundary** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Fig.3 (using plt.scatter)\n",
    "\n",
    "#Sugestion how to plot the decision boundary (the green line)\n",
    "x_value= np.array([np.min(X[:,1]),np.max(X[:,1])])\n",
    "y_value=-(theta[0] +theta[1]*x_value)/theta[2]\n",
    "plt.plot(x_value,y_value, \"g\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "For a student with Exam1 score of 45 and Exam2 score of 85, use the learned model to compute what is the admission probability of this student. The answer is around 77% probability (0.767). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([45,85])\n",
    "#Normalize the values\n",
    "x_test = (x_test - X_mean)/X_std\n",
    "#Add one\n",
    "x_test = np.append(np.ones(1),x_test)\n",
    "#Compute the prediction (the probability for admission)\n",
    "prob = sigmoid(x_test.dot(theta))\n",
    "\n",
    "print(\"For a student with scores 45 and 85, we predict an admission probability of\",prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on training set \n",
    "\n",
    "Evaluate how well the learned model predicts on the training set. \n",
    "\n",
    "Your task is to complete the function *classifierPredict*. \n",
    "\n",
    "The *classifierPredict* function returns a boolean array with True if the probability of admission into university is more than 0.5 and False otherwise. Taking the sum(p==y) adds up all instances where it correctly predicts the given y values (the labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifierPredict(theta,X):\n",
    "    \"\"\"\n",
    "    take in numpy array of theta and X and predict the class \n",
    "    \"\"\"\n",
    "    predictions = X.dot(theta)\n",
    "    \n",
    "    return predictions>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=classifierPredict(theta,X)\n",
    "print(\"Train Accuracy:\", sum(p==y)[0],\"%\")  #ANSWER: Train Accuracy: 89 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
